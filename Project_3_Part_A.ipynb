{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "         \n",
    "For Part A, you need to scrape IMDB web page to find out top movies sorted by user votes. For each movie, you need to pull :\n",
    "- movie_id\n",
    "- rank\n",
    "- title \n",
    "- runtime\n",
    "- year\n",
    "- rating\n",
    "- votes\n",
    "\n",
    "The URL of an page that include movies released between 2018 and 2020 sorted by number of votes is: \n",
    "\n",
    "https://www.imdb.com/search/title/?title_type=feature&release_date=2018-01-01,2020-12-31&sort=num_votes,desc\n",
    "\n",
    "Please click the URL and investigate how you can pull movie_id, rank, title,... from the webpage. \n",
    "\n",
    "\n",
    "**You need to write code after where I have <span style=\"color:red\">'''  Your code here ...    '''.</span>**\n",
    "\n",
    "***\n",
    "Now let’s look at the read_m_from_html_string(url, num_of_m=50) function in detail. The parameter “num_of_m” in the function def read_m_from_html_string(url, num_of_m=50)\n",
    "  represents the top number of movies you want to retrieve. For example, read_m_from_html_string(url,500) means that we want to extract top 500 movies released between, sorted by users' votes.\n",
    "\n",
    "This function returns a list of dictionaries. Each dictionary represents one of the top movies, which could look like the following:\n",
    "\n",
    "{\n",
    "  \n",
    "    'movie_id': 'tt7286456',\n",
    "    'rank': '1.',\n",
    "    'title': 'Joker',\n",
    "    'runtime': 2h 2m,\n",
    "    'year': '2019',\n",
    "    'rating': '8.4',\n",
    "    'votes': '1,421,777',\n",
    "}\n",
    "\n",
    "\n",
    "After you implement “read_m_from_html_string”, which will return a list of top movies, you need to export the movies list to a csv file.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "After you done with scraping the needed data, you should clean and transform it as needed to make it ready for enriching the given \"Movies.csv\" dataset.\n",
    "***\n",
    "\n",
    "Finaly, export the enriched dataset to a CSV file:\n",
    "Use the following naming convention: Project_3_PartA_Lastname.csv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## read_m_from_html_string\n",
    "\n",
    "Inside this function, you need to write your code to pull the movies information from the provided Movies 500 HTML String text file.\n",
    "\n",
    "For each movie, you need to pull :\n",
    "- movie_id\n",
    "- rank\n",
    "- title \n",
    "- runtime\n",
    "- year\n",
    "- rating\n",
    "- votes\n",
    "\n",
    "To give examples on how to pull data from the web bage html string, I have included the code to pull the movie_id.\n",
    "You need to inculde your code to pull the other needed movie information (title, rank, year, ......). You should have no missing values for each of the collected data.\n",
    "\n",
    "The URL of an page that include movies released between 2018 and 2020 sorted by number of votes is: \n",
    "\n",
    "https://www.imdb.com/search/title/?title_type=feature&release_date=2018-01-01,2020-12-31&sort=num_votes,desc\n",
    "\n",
    "Please click the URL and investigate how you can pull movie_id, rank, title,... from the webpage using the Inspect option.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function, read a number of movies from a url html string. The default value is 50\n",
    "def read_m_from_html_string(url, num_of_m=50):\n",
    "    \n",
    "    print(url)\n",
    "    \n",
    "    with open('TopVoted_500_Movies_HTML.txt', 'r', encoding=\"utf8\") as file:\n",
    "        html_string = file.read()   # to read the hmtl file as a string\n",
    "        # I have included the Movies 500 HTML String.txt file in the project folder. Please take a look.\n",
    "    \n",
    "    # create a soup object\n",
    "    soup = BeautifulSoup(html_string, \"html.parser\")\n",
    "    \n",
    "    '''\n",
    "    Click the URL and investigate how you can pull movie_id, rank, title,... from the webpage.\n",
    "    To investigate the html of a web page , For example:\n",
    "    URL: https://www.imdb.com/search/title/?title_type=feature&release_date=2018-01-01,2020-12-31&sort=num_votes,desc\n",
    "    Right-click anywhere on the webpage, and at the very bottom of the menu that pops up, \n",
    "    you will see \"Inspect\", Click on it.\n",
    "    '''\n",
    "    '''\n",
    "    Fetching a div that includes all the movies. This can be done by using find and find_all functions.\n",
    "    for example, find_all('div') will give you all divs on the page. Actually, \n",
    "    this find or find_all function can have two parameters,\n",
    "    in the code below 'div' is the tag name and 'ipc-page-grid__item ipc-page-grid__item--span-2' is an attribute \n",
    "    value of the tag. You can also do movie_list = soup.find('div', 'ipc-page-grid__item ipc-page-grid__item--span-2'). \n",
    "    Here you explicitly say: I want to find a div with \n",
    "    attribute class = 'ipc-page-grid__item ipc-page-grid__item--span-2'.\n",
    "    \n",
    "    Since on each imdb page, there's only one div with class = 'lister-list', we can use find rather than find_all. \n",
    "    Find_all will return a list of div tags, while find() will return only one div.\n",
    "   '''     \n",
    "    movie_list = soup.find('div', 'ipc-page-grid__item ipc-page-grid__item--span-2') \n",
    "    # this div contains all the listed movies in the requested html web page string.\n",
    "    \n",
    "    list_movies = [] # initialize the function return value, which is a list of movies. \n",
    "                     # This list will contains the scraped data transformed to a structured format.\n",
    "    \n",
    "    # Using count track the number of movies processed. now it's 0 - No movie has been processed yet.\n",
    "    count = 0\n",
    "    \n",
    "    # each movie listed in a div with attribute value 'ipc-metadata-list-summary-item'.\n",
    "    divs=  movie_list.find_all('li','ipc-metadata-list-summary-item') # To find all the listed movies in the page.\n",
    "    for d in divs:\n",
    "        dict_each_movie = {}  # initialize the movie dictionary to store the movie information.\n",
    "\n",
    "        # Pulling the movie_id\n",
    "        try:\n",
    "            movie_id= d.find('a', 'ipc-title-link-wrapper').attrs['href']\n",
    "            movie_id= movie_id[7:16]\n",
    "            \n",
    "        except:\n",
    "            movie_id=\"\"\n",
    "        finally:\n",
    "            dict_each_movie[\"movie_id\"] = movie_id\n",
    "            print(movie_id)\n",
    "            \n",
    "        # Pulling the rank\n",
    "        '''  Your code here ...    '''\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # Pulling the title\n",
    "        '''  Your code here ...    '''\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Pulling the runtime\n",
    "        '''  Your code here ...    '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Pulling the year\n",
    "        '''  Your code here ...    ''' \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Pulling the rating\n",
    "          # the rating out of 10\n",
    "        '''  Your code here ...    '''     \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        # Pulling the votes\n",
    "        '''  Your code here ...    '''\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        list_movies.append(dict_each_movie)  # To add the movie information to the movies list.\n",
    "\n",
    "        count +=1\n",
    "        print('===============================')\n",
    "        print()\n",
    "        if count == num_of_m:\n",
    "            break # to exit from the loop.\n",
    "\n",
    "    return list_movies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Call statement to scrap the TopVoted 500 movies\n",
    "##### read_m_from_html_string(url,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.imdb.com/search/title/?title_type=feature&release_date=2018-01-01,2020-12-31&sort=num_votes,desc\"\n",
    "\n",
    "Movies_list = read_m_from_html_string(url,500)  #to read the topVoted 500 movies\n",
    "Movies_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert the movies list of dics to dataframe\n",
    "df_movies = pd.DataFrame(Movies_list)\n",
    "df_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#  To export the colleted movies to IMDb_TopVoted.csv file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies.to_csv('IMDb_TopVoted.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the given dataset \"Movies.csv\" to Pandas DataFrame called df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the csv file to df1 and print the df1.\n",
    "\n",
    "'''  Your code here ...    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the scraped data from the IMDb_TopVoted.csv file to Pandas DataFrame called df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You need to import the collected dataset \"IMDb_TopVoted.csv\" and print the df2.\n",
    "# To handel Latin characters that may contained in the csv file\n",
    "# with no issue, use  encoding= \"ISO-8859-1\" with the pd.read_csv()\n",
    "# Example: df1 = pd.read_csv('thefilename.csv', encoding= \"ISO-8859-1\") \n",
    "# Using encoding= \"ISO-8859-1\" will avoid Unicode-Decode-Errors.\n",
    "\n",
    "'''  Your code here ...    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleansing and transformation for df2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cleaning and tranforming df2\n",
    " # rank, year, and votes should have a numeric integer data type.\n",
    " # runtime column should be renamed to runtimeMinutes and the value should be in minutes, \n",
    " # for example: 2h 2m should be 122\n",
    "    \n",
    "'''  Your code here ...    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \tEnrich the given dataset (df1) by merging it to the scraped data (df2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merege the two dataframes to one dataframe called df.\n",
    "'''  Your code here ...    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rearrange the dataset fields to be listed in the following order: \n",
    "movie_id , rank , title ,  originalTitle ,  description ,\n",
    "          year ,  votes , rating ,  runtimeMinutes ,  ratingCategory ,  genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange the dataset fields.\n",
    "'''  Your code here ...    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the enriched dataset to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following naming convention: \n",
    "#  Project_3_PartA_Lastname.csv\n",
    "'''  Your code here ...    '''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
